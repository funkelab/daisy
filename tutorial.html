

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Daisy Tutorial &mdash; daisy  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference" href="api.html" />
    <link rel="prev" title="Daisy: A Blockwise Task Scheduler" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            daisy
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Daisy Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Building-Blocks">Building Blocks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#A-Simple-Example:-Local-Smoothing">A Simple Example: Local Smoothing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Distributing-on-the-Cluster">Distributing on the Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Important-Features">Important Features</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Some-Minimal-Examples">Some Minimal Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Introduction-and-overview">Introduction and overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Environment-setup">Environment setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Multiprocessing">Multiprocessing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Daisy-Tasks">Daisy Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#A-Simple-Task">A Simple Task</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Failing-blocks">Failing blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Different-read/write-sizes-and-boundary-handling">Different read/write sizes and boundary handling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Task-Chaining">Task Chaining</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#Map-Reduce-and-Some-simple-benchmarks">Map Reduce and Some simple benchmarks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Simple-data">Simple data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Map">Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Reduce">Reduce</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="release.html">Release notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">daisy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Daisy Tutorial</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tutorial.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><br/><span></span># before we start, we are setting the start method to fork for convenience
# and for running in a jupyter notebook. This is not always necessary.
import multiprocessing

multiprocessing.set_start_method(&quot;fork&quot;, force=True)
</pre></div>
</div>
</div>
<section id="Daisy-Tutorial">
<h1>Daisy Tutorial<a class="headerlink" href="#Daisy-Tutorial" title="Link to this heading"></a></h1>
<p>Daisy is a library for processing large volumes in parallel. While other libraries (e.g. dask) can perform similar tasks, daisy is optimized for extremely large volumes, out-of-memory operations, and operations where neighboring blocks should not be run at the same time.</p>
<p>In this tutorial, we will cover:</p>
<ul class="simple">
<li><p>daisy terminology and concepts</p></li>
<li><p>running daisy locally with multiprocessing</p></li>
<li><p>running daisy with independent worker processes (e.g., on a compute cluster)</p></li>
<li><p>key features of daisy that make it unique</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Don&#39;t forget to install daisy with dependencies for the docs before running this tutorial:
#
# `pip install daisy[docs]``
</pre></div>
</div>
</div>
<section id="Building-Blocks">
<h2>Building Blocks<a class="headerlink" href="#Building-Blocks" title="Link to this heading"></a></h2>
<p>Daisy is designed for processing volumetric data. Therefore, it has specific ways to describe locations in a volume. We will demonstrate the common terms and utilities using this image of astronaut Eileen Collins.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>from skimage import data
import numpy as np
import matplotlib.pyplot as plt

raw_data = np.flip(data.astronaut(), 0)
axes_image = plt.imshow(raw_data, zorder=1, origin=&quot;lower&quot;)
</pre></div>
</div>
</div>
<p>###Coordinate</p>
<ul class="simple">
<li><p>A daisy Coordinate is essentially a tuple with one value per spatial dimension. In our case, the Coordinates are two-dimensional.</p></li>
<li><p>Daisy coordinates can represent points in the volume, or distances in the volume.</p></li>
<li><p>Daisy mostly passes around abstract placeholders of the data. Therefore, a Coordinate does not contain data, it is simply a pointer to a location in a volume.</p></li>
<li><p>The main difference between a Coordinate and a tuple is that operations (e.g. addition) between Coordinates are performed pointwise, to match the spatial definition.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">daisy.Coordinate</span></code> is an alias of <code class="docutils literal notranslate"><span class="pre">`funlib.geometry.Coordinate</span></code> &lt;<a class="reference external" href="https://github.com/funkelab/funlib.geometry/blob/main/funlib/geometry/coordinate.py">https://github.com/funkelab/funlib.geometry/blob/main/funlib/geometry/coordinate.py</a>&gt;`__</p>
<p>Here are some example Coordinates, and a visualization of their location on the Eileen Collins volume.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import daisy

p1 = daisy.Coordinate(10, 10)  # white
p2 = daisy.Coordinate(452, 250)  # yellow
p3 = p2 - p1  # orange
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def display_coord(axes, coord, color):
    x = coord[1]
    y = coord[0]
    axes.scatter(x, y, color=color, edgecolors=&quot;black&quot;, zorder=3)


figure = axes_image.figure
axes = figure.axes[0]
for point, color in zip([p1, p2, p3], [&quot;white&quot;, &quot;yellow&quot;, &quot;orange&quot;]):
    display_coord(axes, point, color=color)
figure
</pre></div>
</div>
</div>
<p>###Roi A Roi (Region of interest) is a bounding box in a volume. It is defined by two Coordinates:</p>
<ul class="simple">
<li><p>offset: the starting corner of the bounding box relative to the origin</p></li>
<li><p>shape: The extent of the bounding box in each dimension</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">daisy.Roi</span></code> is an alias of <code class="docutils literal notranslate"><span class="pre">`funlib.geometry.Roi</span></code> &lt;<a class="reference external" href="https://github.com/funkelab/funlib.geometry/blob/main/funlib/geometry/roi.py">https://github.com/funkelab/funlib.geometry/blob/main/funlib/geometry/roi.py</a>&gt;`__. Rois have operations like grow, shift, and intersect, that represent spatial manipulations</p>
<p>Here are some example Rois and their visualization in our Eileen Collins volume. Remember, the Roi does not contain the data! It is simply a bounding box.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>head = daisy.Roi(offset=(320, 150), shape=(180, 150))  # purple
# to get the Roi of the nose, we will shrink the head Roi by a certain amount in x and y on each side of the Roi
# the first argument will move the corner closest to the origin, and the second argument will move the corner furthest from the origin
nose = head.grow(daisy.Coordinate(-60, -55), daisy.Coordinate(-90, -55))  # orange

body = daisy.Roi(offset=p1, shape=(330, 350))  # grey
# to get the neck, we will intersect the body and the head rois
neck = head.intersect(body)  # blue
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>from matplotlib.patches import Rectangle


def display_roi(axes, roi, color):
    xy = (roi.offset[1], roi.offset[0])
    width = roi.shape[1]
    height = roi.shape[0]
    rect = Rectangle(xy, width, height, alpha=0.4, color=color, zorder=2)
    axes.add_patch(rect)


def fresh_image():
    plt.close()
    axes_image = plt.imshow(raw_data, zorder=1, origin=&quot;lower&quot;)
    figure = axes_image.figure
    axes = figure.axes[0]
    return figure


figure = fresh_image()
for roi, color in zip([head, nose, body, neck], [&quot;purple&quot;, &quot;orange&quot;, &quot;grey&quot;, &quot;blue&quot;]):
    display_roi(figure.axes[0], roi, color=color)
</pre></div>
</div>
</div>
<p>###Array So far we have seen how to specify regions of the data with Rois and Coordinates, which do not contain any data. However, eventually you will need to access the actual data using your Rois! For this, we use <code class="docutils literal notranslate"><span class="pre">`funlib.persistence.arrays.Array</span></code> &lt;<a class="reference external" href="https://github.com/funkelab/funlib.persistence/blob/f5310dddb346585a28f3cb44f577f77d4f5da07c/funlib/persistence/arrays/array.py">https://github.com/funkelab/funlib.persistence/blob/f5310dddb346585a28f3cb44f577f77d4f5da07c/funlib/persistence/arrays/array.py</a>&gt;`__. If you are familiar with dask, this is the daisy equivalent of dask arrays.</p>
<p>The core information about the <code class="docutils literal notranslate"><span class="pre">funlib.persistence.arrays.Array</span></code> class is that you can slice them with Rois, along with normal numpy-like slicing. However, in order to support this type of slicing, we need to also know the Roi of the whole Array. Here we show you how to create an array from our raw data that is held in memory as a numpy array. However, we highly recommend using a zarr backend, and will show this in our simple example next!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>from funlib.persistence.arrays import Array

# daisy Arrays expect the channel dimension first, but our sklearn loaded image has channels last - let&#39;s fix that
raw_data_reshaped = raw_data.transpose(2, 0, 1)
print(&quot;New data shape:&quot;, raw_data_reshaped.shape)

# we need the spatial extend of the data
data_spatial_shape = raw_data_reshaped.shape[1:]
print(&quot;Spatial shape:&quot;, data_spatial_shape)

# Roi of the whole volume
total_roi = daisy.Roi(offset=(0, 0), shape=data_spatial_shape)
print(&quot;Total dataset roi:&quot;, total_roi)

raw_array = Array(
    data=raw_data_reshaped,
    offset=total_roi.offset,
    voxel_size=daisy.Coordinate(1, 1),
)
</pre></div>
</div>
</div>
<p>Now we can demonstrate how to access data from an Array using a Roi</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># slicing an Array with a Roi gives you a numpy array
head_data = raw_array[head]
plt.close()
plt.imshow(
    head_data.transpose(1, 2, 0), origin=&quot;lower&quot;
)  # need to transpose channels back to the end for matplotlib to work
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># you can also combine the two steps
body_data = raw_array.to_ndarray(body)
plt.close()
plt.imshow(body_data.transpose(1, 2, 0), origin=&quot;lower&quot;)
</pre></div>
</div>
</div>
<p>###Block</p>
<p>Daisy is a blockwise task scheduler. Therefore, the concept of a block is central to Daisy. To efficiently process large volumes, Daisy splits the whole volume into a set of adjacent blocks that cover the whole image. These blocks are what is passed between the scheduler and the workers.</p>
<p>A Block is simply a (set of) Roi(s), and does not contain data. In practice, it has additional information that is useful to the daisy server and workers to help them perform their task, which we will decribe below:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># let&#39;s define a Block of our Eileen Collins volume
# In practice, you will never need to make a block - the daisy scheduler will do this for you

block_size = daisy.Coordinate(64, 64)
block_origin = daisy.Coordinate(128, 128)
block_roi = daisy.Roi(block_origin, block_size)

block = daisy.Block(
    total_roi=total_roi,
    read_roi=block_roi,
    write_roi=block_roi,
)

# Here are all the attributes of the block
print(
    &quot;Block id:&quot;, block.block_id
)  # a unique ID for each block given the total roi of the volume
print(
    &quot;Block read roi:&quot;, block.read_roi
)  # the Roi which represents the location in the volume of the input data to the process
print(
    &quot;Block write roi:&quot;, block.write_roi
)  # the Roi which represents the location in the volume where the process should write the output data
print(
    &quot;Block status:&quot;, block.status
)  # The status of the block (e.g. created, in progress, succeeded, failed)

# let&#39;s look at the write roi of our block on top of the original figure
figure = fresh_image()
display_roi(figure.axes[0], block.write_roi, color=&quot;white&quot;)
</pre></div>
</div>
</div>
<p>You may be wondering why the block has a read roi and a write roi - this will be illustrated next in our simple daisy example!</p>
</section>
<section id="A-Simple-Example:-Local-Smoothing">
<h2>A Simple Example: Local Smoothing<a class="headerlink" href="#A-Simple-Example:-Local-Smoothing" title="Link to this heading"></a></h2>
<p>In this next example, we will use gaussian smoothing to illustrate how to parallize a task on your local machine using daisy.</p>
<p>###Dataset Preparation As mentioned earlier, we highly recommend using a zarr/n5 backend for your volume. Daisy is designed such that no data is transmitted between the worker and the scheduler, including the output of the processing. That means that each worker is responsible for saving the results in the given block write_roi. With a zarr backend, each worker can write to a specific region of the zarr in parallel, assuming that the chunk size is a divisor of and aligned with the write_roi. The
zarr dataset must exist before you start scheduling though - we recommend using <code class="docutils literal notranslate"><span class="pre">`funlib.persistence.prepare_ds</span></code> &lt;<a class="reference external" href="https://github.com/funkelab/funlib.persistence/blob/f5310dddb346585a28f3cb44f577f77d4f5da07c/funlib/persistence/arrays/datasets.py#L423">https://github.com/funkelab/funlib.persistence/blob/f5310dddb346585a28f3cb44f577f77d4f5da07c/funlib/persistence/arrays/datasets.py#L423</a>&gt;`__ function to prepare the dataset. Then later, you can use <code class="docutils literal notranslate"><span class="pre">`funlib.persistence.open_ds</span></code> &lt;<a class="reference external" href="https://github.com/funkelab/funlib.persistence/blob/f5310dddb346585a28f3cb44f577f77d4f5da07c/funlib/persistence/arrays/datasets.py#L328">https://github.com/funkelab/funlib.persistence/blob/f5310dddb346585a28f3cb44f577f77d4f5da07c/funlib/persistence/arrays/datasets.py#L328</a>&gt;`__ to open the
dataset and it will automatically read the metadata and wrap it into a <code class="docutils literal notranslate"><span class="pre">funlib.persistence.Array</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import zarr

# convert our data to float, because gaussian smoothing in scikit expects float input and output
# recall that we already reshaped it to put channel dimension first, as the funlib.persistence Array expects
raw_data_float = raw_data_reshaped.astype(np.float32) / 255.0

# store image in zarr container
f = zarr.open(&quot;sample_data.zarr&quot;, &quot;w&quot;)
f[&quot;raw&quot;] = raw_data_float
f[&quot;raw&quot;].attrs[&quot;offset&quot;] = daisy.Coordinate((0, 0))
f[&quot;raw&quot;].attrs[&quot;resolution&quot;] = daisy.Coordinate(
    (1, 1)
)  # this attribute holds the voxel size
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>from funlib.persistence import prepare_ds
# prepare an output dataset with a chunk size that is a divisor of the block roi

n_channels = 3  # our output will be an RGB image as well
prepare_ds(
    &quot;sample_data.zarr/smoothed&quot;,
    shape=(n_channels, *total_roi.shape),
    offset=total_roi.offset,  # if your output has a different total_roi than your input, you would need to change this
    voxel_size=daisy.Coordinate((1, 1)),
    dtype=raw_data_float.dtype,
    # The write size is important! If you don&#39;t set this correctly, your workers will have race conditions
    # when writing to the same file, resulting in expected behavior or weird errors.
    # The prepare_ds function takes care of making zarr chunk sizes that evenly divide your write size
    chunk_shape=(n_channels, *block_size),
)
print(&quot;Shape of output dataset:&quot;, f[&quot;smoothed&quot;].shape)
print(&quot;Chunk size in output dataset:&quot;, f[&quot;smoothed&quot;].chunks)
</pre></div>
</div>
</div>
<p>###Define our Process Function When run locally, daisy process functions must take a block as the only argument. Depending on the multiprocessing spawn function settings on your computer, the function might not inherit the imports and variables of the scope where the scheudler is run, so it is always safer to import and define everything inside the function.</p>
<p>Here is an example for smoothing. Generally, daisy process functions have the following three steps:</p>
<ol class="arabic simple">
<li><p>Load the data from disk</p></li>
<li><p>Process the data</p></li>
<li><p>Save the result to disk</p></li>
</ol>
<p>Note that for now, the worker has to know where to load the data from and save the result to. Later we will show you ways around the rule that the process function must only take the block as input, to allow you to pass that information in when you start the scheduler.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def smooth(block: daisy.Block):
    # imports and hyperaparmeters inside scope, to be safe
    from funlib.persistence.arrays import open_ds
    from skimage import filters

    sigma = 5.0

    # open the raw dataset as an Array
    raw_ds = open_ds(
        &quot;sample_data.zarr/raw&quot;,
        &quot;r&quot;,
    )
    # Read the data in the block read roi and turn it into a numpy array
    data = raw_ds.to_ndarray(block.read_roi)
    # smooth the data using the gaussian filter from skimage
    smoothed = filters.gaussian(data, sigma=sigma, channel_axis=0)
    # open the output smoothed dataset as an Array
    output_ds = open_ds(&quot;sample_data.zarr/smoothed&quot;, &quot;a&quot;)
    # save the result in the output dataset using the block write roi
    output_ds[block.write_roi] = smoothed
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># Let&#39;s test the data on our block that we defined earlier and visualize the result
smooth(block)
plt.imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;smoothed&quot;][:].transpose(1, 2, 0), origin=&quot;lower&quot;
)
</pre></div>
</div>
</div>
<p>###Run daisy with local multiprocessing We are about ready to run daisy! We need to tell the scheduler the following pieces of information:</p>
<ul class="simple">
<li><p>The process function, which takes a block as an argument</p></li>
<li><p>The total roi to process (in our case, the whole image)</p></li>
<li><p>The read roi and write roi of each block (the shape and relative offset are what is important, since they will be shifted as a pair to tile the total_roi)</p></li>
<li><p>How many workers to spawn</p></li>
</ul>
<p>These pieces of information get wrapped into a <code class="docutils literal notranslate"><span class="pre">`daisy.Task</span></code> &lt;<a class="reference external" href="https://github.com/funkelab/daisy/blob/master/daisy/task.py">https://github.com/funkelab/daisy/blob/master/daisy/task.py</a>&gt;`__, along with a name for the task. Then the <code class="docutils literal notranslate"><span class="pre">daisy.run_blockwise</span></code> function starts the scheduler, which creates all the blocks that tile the total roi, spawns the workers, distributes the blocks to the workers, and reports if the blocks were successfully processed.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>daisy.run_blockwise(
    [
        daisy.Task(
            &quot;Smoothing&quot;,  # task name
            process_function=smooth,  # a function that takes a block as argument
            total_roi=total_roi,  # The whole roi of the image
            read_roi=block_roi,  # The roi that the worker should read from
            write_roi=block_roi,  # the roi that the worker should write to
            num_workers=5,
        )
    ]
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plt.imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;smoothed&quot;][:].transpose(1, 2, 0), origin=&quot;lower&quot;
)
</pre></div>
</div>
</div>
<p>###Take 2: Add context! The task ran successfully, but you’ll notice that there are edge artefacts where the blocks border each other. This is because each worker only sees the inside of the block, and it needs more context to smooth seamlessly between blocks. If we increase the size of the read_roi so that each block sees all pixels that contribute meaningfully to the smoothed values in the interior (write_roi) of the block, the edge artefacts should disappear.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sigma = 5
context = 2 * sigma  # pixels beyond 2*sigma contribute almost nothing to the output
block_read_roi = block_roi.grow(context, context)
block_write_roi = block_roi
# we also grow the total roi by the context, so that the write_rois are still in the same place when we tile
total_read_roi = total_roi.grow(context, context)

block = daisy.Block(
    total_roi=total_roi,
    read_roi=block_read_roi,
    write_roi=block_write_roi,
)

# let&#39;s look at the new block rois
figure = fresh_image()
display_roi(figure.axes[0], block.read_roi, color=&quot;purple&quot;)
display_roi(figure.axes[0], block.write_roi, color=&quot;white&quot;)
</pre></div>
</div>
</div>
<p>Let’s prepare another dataset to store our new and improved smoothing result in. We will be doing this repeatedly through the rest of the tutorial, so we define a helper function to prepare a smoothing result in a given group in the <code class="docutils literal notranslate"><span class="pre">sample_data.zarr</span></code>. We also define a helper function for deleting a dataset, in case you want to re-run a processing step and see a new result.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def prepare_smoothing_ds(group):
    prepare_ds(
        f&quot;sample_data.zarr/{group}&quot;,
        shape=(3, *total_roi.shape),
        offset=total_roi.offset,
        voxel_size=daisy.Coordinate((1, 1)),
        dtype=raw_data_float.dtype,
        chunk_shape=(3, *block_size),
    )


def delete_ds(group):
    root = zarr.open(&quot;sample_data.zarr&quot;, &quot;a&quot;)
    if group in root:
        del root[group]


output_group = &quot;smoothed_with_context&quot;
prepare_smoothing_ds(output_group)
</pre></div>
</div>
</div>
<p>Now we have to adapt our process function to crop the output before saving. It would be nice to be able to pass the output group in as an argument, so we will show you a workaround using <code class="docutils literal notranslate"><span class="pre">functools.partial</span></code> to partially evaluate the function. To use this workaround, your process function must have the block as the last argument.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def smooth_in_block(output_group: str, block: daisy.Block):
    # imports and hyperaparmeters inside scope, to be safe
    from funlib.persistence.arrays import open_ds, Array
    from skimage import filters
    import time

    sigma = 5.0
    # open the raw dataset as an Array
    raw_ds = open_ds(
        &quot;sample_data.zarr/raw&quot;,
        &quot;r&quot;,
    )
    # Read the data in the block read roi and turn it into a numpy array
    data = raw_ds.to_ndarray(
        block.read_roi, fill_value=0
    )  # NOTE: this fill value allows you to read outside the total_roi without erroring
    # smooth the data using the gaussian filter from skimage
    smoothed = filters.gaussian(data, sigma=sigma, channel_axis=0)
    # open the output smoothed dataset as an Array
    output_ds = open_ds(f&quot;sample_data.zarr/{output_group}&quot;, &quot;a&quot;)
    # turn the smoothed result into an Array so we can crop it with a Roi (you can also center crop it by the context manually, but this is easier!)
    smoothed = Array(smoothed, offset=block.read_roi.offset, voxel_size=(1, 1))
    # save the result in the output dataset using the block write roi
    output_ds[block.write_roi] = smoothed.to_ndarray(block.write_roi)
</pre></div>
</div>
</div>
<p>Now we can re-run daisy. Note these changes from the previous example:</p>
<ul class="simple">
<li><p>using <code class="docutils literal notranslate"><span class="pre">functools.partial</span></code> to partially evaluate our <code class="docutils literal notranslate"><span class="pre">smooth_in_block</span></code> function , turning it into a function that only takes the block as an argument</p></li>
<li><p>the total_roi is now expanded to include the context, as is the read_roi</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>from functools import partial

daisy.run_blockwise(
    [
        daisy.Task(
            &quot;Smoothing with context&quot;,
            process_function=partial(smooth_in_block, output_group),
            total_roi=total_read_roi,
            read_roi=block_read_roi,
            write_roi=block_write_roi,
            num_workers=5,
        )
    ],
    multiprocessing=False,
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plt.imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;smoothed_with_context&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
</pre></div>
</div>
</div>
<p>Success! Notice that there is a fade to black at the border, due to the <code class="docutils literal notranslate"><span class="pre">fill_value=0</span></code> argument used when reading the data from the input Array. Smoothing is poorly defined at the border of the volume - if you want different behavior, you can expand the input array to include extended data of your choice at the border, or shrink the total output roi by the context to only include the section of the output that depends on existing data.</p>
<p>###Conclusion: Dask and Daisy</p>
<p>Congrats! You have learned the basics of Daisy. In this example, we only parallelized the processing using our local computer’s resources, and our “volume” was very small.</p>
<p>If your task is similar to this example, you can use dask to do the same task with many fewer lines of code:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>%pip install dask
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import dask
import dask.array as da
from skimage import filters


f = zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)
raw = da.from_array(f[&quot;raw&quot;], chunks=(3, 64, 64))
print(&quot;Raw dask array:&quot;, raw)
sigma = 5.0
context = int(sigma) * 2


def smooth_in_block_dask(x):
    return filters.gaussian(x, sigma=sigma, channel_axis=0)


smoothed = raw.map_overlap(smooth_in_block_dask, depth=(0, context, context))
plt.imshow(smoothed.transpose((1, 2, 0)), origin=&quot;lower&quot;)
</pre></div>
</div>
</div>
<p>For many situations, both dask and daisy can work well. Indeed, for some tasks, dask is simpler and better suited, as it does for you many things that daisy leaves to you to implement. One key difference between dask and daisy is that in dask, functions are not supposed to have side effects. In daisy, functions can have side effects, allowing blocks to depend on other blocks in the scheduling order (see the last two examples in this tutorial about task chaining and read-write conflicts).</p>
<p>In general, daisy is designed for…</p>
<ul class="simple">
<li><p>Cases where you want to be able to pick up where you left off after an error, rather than starting the whole task over (because blocks that finished saved their results to disk)</p></li>
<li><p>Cases where blocks should be executed in a particular order, so that certain blocks see other blocks outputs (without passing the output through the scheduler)</p></li>
<li><p>Cases where the worker function needs setup and teardown that takes longer than processing a block (see our next example!)</p></li>
</ul>
</section>
<section id="Distributing-on-the-Cluster">
<h2>Distributing on the Cluster<a class="headerlink" href="#Distributing-on-the-Cluster" title="Link to this heading"></a></h2>
<p>While daisy can run locally, it is designed to shine in a cluster computing environment. The only information passed between scheduler and workers are Blocks, which are extremely lightweight and are communicated through TCP. Therefore, workers can be distributed on the cluster with minimal communication overhead.</p>
<p>Let’s re-do our smoothing, but this time run each worker as a completely separate subprocess, as would be needed on a cluster. First, we prepare the output dataset.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># first, prepare the dataset
prepare_smoothing_ds(&quot;smoothed_subprocess&quot;)
</pre></div>
</div>
</div>
<p>Then, we prepare our process function. This time, it has two parts. The first part is the function defined in the cell below, and essentially just calls <code class="docutils literal notranslate"><span class="pre">subprocess.run</span></code> locally or with bsub, as an example compute environment. The second part is the external python script that is actually executed in the <code class="docutils literal notranslate"><span class="pre">subprocess.run</span></code> call.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># new process function to start the worker subprocess
def start_subprocess_worker(cluster=&quot;local&quot;):
    import subprocess

    if cluster == &quot;bsub&quot;:
        # this is where you define your cluster arguments specific to your task (gpus, cpus, etc)
        num_cpus_per_worker = 1
        subprocess.run(
            [
                &quot;bsub&quot;,
                &quot;-I&quot;,
                f&quot;-n {num_cpus_per_worker}&quot;,
                &quot;python&quot;,
                &quot;./tutorial_worker.py&quot;,
                &quot;tutorial_config.json&quot;,
            ]
        )
    elif cluster == &quot;local&quot;:
        subprocess.run([&quot;python&quot;, &quot;tutorial_worker.py&quot;, &quot;tutorial_config.json&quot;])
    else:
        raise ValueError(&quot;Only bsub and local currently supported for this tutorial&quot;)
</pre></div>
</div>
</div>
<p>Code from tutorial_worker.py, copied here for convenience (Note: running this cell won’t run the code, because it is a markdown cell)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">daisy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">funlib.persistence.arrays</span><span class="w"> </span><span class="kn">import</span> <span class="n">open_ds</span><span class="p">,</span> <span class="n">Array</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">skimage</span><span class="w"> </span><span class="kn">import</span> <span class="n">filters</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>


<span class="c1">## This function is the same as the local function, but we can pass as many different arguments as we want, and we don&#39;t need to import inside it</span>
<span class="k">def</span><span class="w"> </span><span class="nf">smooth_in_block</span><span class="p">(</span><span class="n">block</span><span class="p">:</span> <span class="n">daisy</span><span class="o">.</span><span class="n">Block</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;sigma&quot;</span><span class="p">]</span>
    <span class="n">raw_ds</span> <span class="o">=</span> <span class="n">open_ds</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;input_zarr&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;input_group&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">raw_ds</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">read_roi</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">smoothed</span> <span class="o">=</span> <span class="n">filters</span><span class="o">.</span><span class="n">gaussian</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">output_ds</span> <span class="o">=</span> <span class="n">open_ds</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;output_zarr&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;output_group&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span>
    <span class="n">smoothed</span> <span class="o">=</span> <span class="n">Array</span><span class="p">(</span><span class="n">smoothed</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">block</span><span class="o">.</span><span class="n">read_roi</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="n">voxel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">output_ds</span><span class="p">[</span><span class="n">block</span><span class="o">.</span><span class="n">write_roi</span><span class="p">]</span> <span class="o">=</span> <span class="n">smoothed</span><span class="o">.</span><span class="n">to_ndarray</span><span class="p">(</span><span class="n">block</span><span class="o">.</span><span class="n">write_roi</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># load a config path or other parameters from the sysargs (recommended to use argparse argument parser for anything more complex)</span>
    <span class="n">config_path</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># load the config</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># simulate long setup time (e.g. loading a model)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># set up the daisy client (this is done by daisy automatically in the local example)</span>
    <span class="c1"># it depends on environment variables to determine configuration</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">daisy</span><span class="o">.</span><span class="n">Client</span><span class="p">()</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># ask for a block from the scheduler</span>
        <span class="k">with</span> <span class="n">client</span><span class="o">.</span><span class="n">acquire_block</span><span class="p">()</span> <span class="k">as</span> <span class="n">block</span><span class="p">:</span>

            <span class="c1"># The scheduler will return None when there are no more blocks left</span>
            <span class="k">if</span> <span class="n">block</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># process your block!</span>
            <span class="c1"># Note: you can now define whatever function signature you want, rather than being limited to one block argument</span>
            <span class="n">smooth_in_block</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>The most important thing to notice about the new worker script is the use of the <code class="docutils literal notranslate"><span class="pre">client.acquire_block()</span></code> function. If you provide a process function that takes a block as input, as we did previously, daisy will create the <code class="docutils literal notranslate"><span class="pre">daisy.Client</span></code>, <code class="docutils literal notranslate"><span class="pre">while</span></code> loop, and <code class="docutils literal notranslate"><span class="pre">client.acquire_block()</span></code> context for you. If you provide a process function with no arguments, the worker is expected to set up the client and request blocks.</p>
<p>Doing the <code class="docutils literal notranslate"><span class="pre">daisy.Client</span></code> set up yourself is helpful when worker startup is expensive - loading saved network weights can be more expensive than actually predicting for one block, so you definitely would not want to load the model separately for each block. We have simulated this by using time.sleep() in the setup of the worker, so when you run the next cell, it should take 20 seconds to start up and then the blocks should process quickly after that.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># note: Must be on submit node to run this with bsub argument
# For Janelians: Don&#39;t use the login node to run the scheduler!
#     Instead, use the submit node, which can handle more computational load
tutorial_task = daisy.Task(
    &quot;smoothing_subprocess&quot;,
    total_roi=total_read_roi,
    read_roi=block_read_roi,
    write_roi=block_write_roi,
    process_function=partial(start_subprocess_worker, &quot;local&quot;),
    num_workers=2,
)

daisy.run_blockwise([tutorial_task])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plt.imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;smoothed_subprocess&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
</pre></div>
</div>
</div>
</section>
<section id="Important-Features">
<h2>Important Features<a class="headerlink" href="#Important-Features" title="Link to this heading"></a></h2>
<p>There are a few more features that you should know about to take full advantage of daisy!</p>
<p>###Fault tolerance and the pre-check function</p>
<p>Even if your code is completely bug-free, things will always go wrong eventually when scaling up to millions of workers. Perhaps your node on the cluster was shared with another process that temporarily hogged too much memory, or you forgot to set a longer timeout and the process was killed after 8 hours. Let’s see how daisy can help you handle these issues, by seeing what happens when we add random failures to our smoothing task.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># as always, prepare a new output smoothing  dataset
prepare_smoothing_ds(&quot;fault_tolerance&quot;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># simulate failing 50% of the time
def smooth_in_block_with_failure(block: daisy.Block):
    import random
    from funlib.persistence.arrays import open_ds, Array
    from skimage import filters

    if random.random() &lt; 0.5:
        raise ValueError(&quot;Simulating random failure&quot;)

    sigma = 5.0

    raw_ds = open_ds(
        &quot;sample_data.zarr/raw&quot;,
        &quot;r&quot;,
    )
    data = raw_ds.to_ndarray(block.read_roi, fill_value=0)
    smoothed = filters.gaussian(data, sigma=sigma, channel_axis=0)

    output_ds = open_ds(&quot;sample_data.zarr/fault_tolerance&quot;, &quot;a&quot;)

    smoothed = Array(smoothed, offset=block.read_roi.offset, voxel_size=(1, 1))
    output_ds[block.write_roi] = smoothed.to_ndarray(block.write_roi)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>sigma = 5
context = int(sigma) * 2
read_roi = block_roi.grow(context, context)

daisy.run_blockwise(
    [
        daisy.Task(
            &quot;fault tolerance test&quot;,
            process_function=smooth_in_block_with_failure,
            total_roi=total_read_roi,
            read_roi=read_roi,
            write_roi=block_roi,
            read_write_conflict=False,
            num_workers=5,
        )
    ]
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plt.imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;fault_tolerance&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
</pre></div>
</div>
</div>
<p>Debugging multi-process code is inherently difficult, but daisy tries to provide as much information as possible. First, you see the progress bar, which also reports the number of blocks at each state, including failed blocks. Any worker error messages are also logged to the scheduler log, although not the full traceback. Upon completion, daisy provides an error summary, which informs you of the final status of all the blocks, and points you to the full output and error logs for each worker,
which can be found in <code class="docutils literal notranslate"><span class="pre">daisy_logs/&lt;task_name&gt;</span></code>. The worker error log will contain the full traceback for debugging the exact source of an error.</p>
<p>You may have noticed that while we coded the task to fail 50% of the time, much more than 50% of the blocks succeeded. This is because daisy by default retries each block 3 times (on different workers) before marking it as failed, to deal gracefully with random error. We will re-run this example, but set max_retries=0 to see the effect of this parameter.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># delete and re-create the dataset, so that we start from zeros again
delete_ds(&quot;fault_tolerance&quot;)
prepare_smoothing_ds(&quot;fault_tolerance&quot;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>daisy.run_blockwise(
    [
        daisy.Task(
            &quot;fault tolerance test&quot;,
            process_function=smooth_in_block_with_failure,
            total_roi=total_read_roi,
            read_roi=read_roi,
            write_roi=block_roi,
            read_write_conflict=False,
            max_retries=0,
            num_workers=5,
        )
    ]
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plt.imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;fault_tolerance&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
</pre></div>
</div>
</div>
<p>Now we should see a success rate closer to what we would expect. You might notice in the logs a message like <code class="docutils literal notranslate"><span class="pre">Worker</span> <span class="pre">hostname=...:port=..:task_id=fault</span> <span class="pre">tolerance</span> <span class="pre">test:worker_id=...</span> <span class="pre">failed</span> <span class="pre">too</span> <span class="pre">many</span> <span class="pre">times,</span> <span class="pre">restarting</span> <span class="pre">this</span> <span class="pre">worker...</span></code>, which shows another way that daisy is robust to pseudo-random errors. If a specific worker fails multiple times, daisy will assume something might have gone wrong with that worker (e.g. GPU memory taken by another process that the cluster scheduler was not aware
of). The scheduler will shut down and restart the worker, and retry the blocks that failed on that worker. So daisy is very robust to random error.</p>
<p>But what about non-random error, like stopping after 8 hours? If you don’t want to re-process all the already processed blocks from a prior run, you can write a function that takes a block and checks if it is complete, and pass it to the scheduler. The scheduler will run this check function on each block and skip the block if the check function returns true.</p>
<p>Here is an example check function for our smoothing task, that checks if there are any non-zero values in the output array</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def check_complete(output_group, block):
    from funlib.persistence.arrays import open_ds
    import numpy as np

    output_ds = open_ds(f&quot;sample_data.zarr/{output_group}&quot;, &quot;r&quot;)
    if np.max(output_ds.to_ndarray(block.write_roi)) &gt; 0:
        return True
    else:
        return False
</pre></div>
</div>
</div>
<p>If we re-run the task, but with the check function provided, you should see in the execution summary that all the blocks that finished before are skipped. You can continue re-running until you reach 100% completion, without ever re-processing those same blocks</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>daisy.run_blockwise(
    [
        daisy.Task(
            &quot;fault tolerance test&quot;,
            process_function=smooth_in_block_with_failure,
            total_roi=total_read_roi,
            read_roi=read_roi,
            write_roi=block_roi,
            read_write_conflict=False,
            max_retries=1,
            num_workers=5,
            check_function=partial(check_complete, &quot;fault_tolerance&quot;),
        )
    ]
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>plt.imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;fault_tolerance&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
</pre></div>
</div>
</div>
<p>Unfortunately, this is a pretty inefficient pre-check function, because you have to actually read the output data to see if the block is completed. Since this will be run on the scheduler on every block before it is passed to a worker, it might not even be faster than just re-processing the blocks (which is at least distributed).</p>
<p>If you plan to have extremely long running jobs that might get killed in the middle, we recommend including a step in your process function after you write out the result of a block, in which you write out the block id to a database or a file. Then, the pre-check function can just check if the block id is in the file system or database, which is much faster than reading the actual data.</p>
<p>###Task chaining</p>
<p>Frequently, image processing pipelines involve multiple tasks, where some tasks depend on the output of other tasks. For example, we have a function to segment out instances of blue objects in an image, and we want to apply it after smoothing. We can define two tasks and run them sequentially in the scheduler. Daisy will even begin the second task as soon as a single block can be run, rather than waiting for the first task to fully complete before starting the second task.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>%pip install opencv-python
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># here is our function to segment blue objects
# it is just thresholding in HSV space, so unlike smoothing, the neighboring pixels do not affect the outcome
# There is probably a nicer way to get the image into hsv space, but this gets the job done!
def segment_blue_objects(input_group, output_group, block):
    import cv2
    from funlib.persistence.arrays import open_ds, Array
    import numpy as np
    import skimage

    # load the data as usual
    input_ds = open_ds(
        f&quot;sample_data.zarr/{input_group}&quot;,
        &quot;r&quot;,
    )
    data = input_ds.to_ndarray(block.read_roi)

    # massage the data into open cv hsv format :/
    back_to_skimage = (data.transpose(1, 2, 0) * 255).astype(np.uint8)
    cv2_image = cv2.cvtColor(
        skimage.util.img_as_ubyte(back_to_skimage), cv2.COLOR_RGB2BGR
    )
    hsv_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2HSV)

    # Define the color range for detection
    lower_blue = np.array([100, 30, 0])
    upper_blue = np.array([150, 255, 255])

    # Threshold the image to get only blue colors
    mask = cv2.inRange(hsv_image, lower_blue, upper_blue)  # this returns 0 and 255
    mask = mask.astype(np.uint32)
    mask = mask // 255  # turn into 0/1 labels

    # give each connected component its own instance segmentation label
    labels = skimage.measure.label(mask)

    # get a unique ID for each element in the whole volume (avoid repeats between blocks)
    max_number_obj = (
        128 * 128
    )  # This number is an upper bound on the maximum number of objects in a block
    block_id_mask = mask * (block.block_id[1] * max_number_obj)
    labels = labels + block_id_mask

    # save the output
    output_ds = open_ds(f&quot;sample_data.zarr/{output_group}&quot;, &quot;a&quot;)
    output_ds[block.write_roi] = labels
</pre></div>
</div>
</div>
<p>Previously, we always defined our <code class="docutils literal notranslate"><span class="pre">daisy.Task</span></code> inside the call to <code class="docutils literal notranslate"><span class="pre">daisy.run_blockwise</span></code>. Now, we need to save our smoothing task in a variable so we can reference it as a dependency our segmentation task.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># as always, prepare the output dataset
delete_ds(&quot;smoothed_for_seg&quot;)
prepare_smoothing_ds(&quot;smoothed_for_seg&quot;)

# same smoothing task as usual, with unqiue output dataset
smoothing_task = daisy.Task(
    &quot;smooth_for_seg&quot;,
    process_function=partial(smooth_in_block, &quot;smoothed_for_seg&quot;),
    total_roi=total_read_roi,
    read_roi=read_roi,
    write_roi=block_roi,
    num_workers=3,
    read_write_conflict=False,
    check_function=partial(check_complete, &quot;smoothed_for_seg&quot;),
)
</pre></div>
</div>
</div>
<p>Next, we make our segmentatation task. Each task can have different hyperaparameters, including block sizes and total rois. For the segmentation, we will double the block size and not add context, since unlike smoothing, thresholding does not depend on neighboring pixels.</p>
<p>Since our instance segmentation output has different output data type, no channels, and a different total output roi, we need to change our arguments to <code class="docutils literal notranslate"><span class="pre">prepare_ds</span></code> for this task.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># you can have different block sizes in different tasks
seg_block_roi = daisy.Roi((0, 0), (128, 128))

delete_ds(&quot;blue_objects&quot;)
prepare_ds(
    &quot;sample_data.zarr/blue_objects&quot;,
    shape=total_roi.shape,
    offset=total_roi.offset,
    voxel_size=daisy.Coordinate((1, 1)),
    dtype=np.uint32,  # This is different! Our labels will be uint32
    write_size=seg_block_roi.shape,  # use the new block roi to determine the chunk size
)

seg_task = daisy.Task(
    &quot;segmentation&quot;,
    process_function=partial(segment_blue_objects, &quot;smoothed_for_seg&quot;, &quot;blue_objects&quot;),
    total_roi=total_roi,  # Note: This task does not have context (yet...?)
    read_roi=seg_block_roi,  # again, no context
    write_roi=seg_block_roi,  # so read and write rois are the same
    read_write_conflict=False,
    num_workers=5,
    upstream_tasks=[
        smoothing_task
    ],  # Here is where we define that this task depends on the output of the smoothing task
)
</pre></div>
</div>
</div>
<p>Note the <code class="docutils literal notranslate"><span class="pre">upstream_tasks</span></code> argument - this is how you tell the scheduler that this task depends on the output of the smoothing task. Then, we can pass both tasks into <code class="docutils literal notranslate"><span class="pre">run_blockwise</span></code>. You should see that the process bar for the segmentation task starts before the smoothing progress bar completes.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>daisy.run_blockwise([smoothing_task, seg_task])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># make a colormap to display the labels as unique colors
# This doesn&#39;t actually guarantee uniqueness, since it wraps the list at some point
import matplotlib.colors as mcolors

colors_list = list(mcolors.XKCD_COLORS.keys())
# colors_list.remove(&quot;black&quot;)
colormap = mcolors.ListedColormap(colors=[&quot;black&quot;] + colors_list * 101)

figure, axes = plt.subplots(1, 2)
axes[0].imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;smoothed_for_seg&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
blue_objs = zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;blue_objects&quot;][:]
axes[1].imshow(blue_objs, origin=&quot;lower&quot;, cmap=colormap, interpolation=&quot;nearest&quot;)
</pre></div>
</div>
</div>
<p>Now we know how to chain tasks together, but we’ve created another issue. If objects crossed block boundaries, they were assigned different IDs. We will address this problem in the next section.</p>
<p>###Process functions that need to read their neighbor’s output (and the “read_write_conflict” flag)</p>
<p>There is a class of problem where it is useful for a block to see the output of its neighboring blocks. Usually, this need comes up when the task performs detection and linking in the same step. To detect an object in a block, it is useful to know if a neighbor has already detected an object that the object in the current block should link to. This is especially useful if there is some sort of continuity constraint, as in tracking objects over time. Even for tasks without a continuity
constraint, like agglomeration of fragments for instance segmentation, performing the detection and linking at the same time can save you an extra pass over the dataset, which matters when the datasets are large.</p>
<p>The example in this tutorial is only to illustrate how daisy implements the ability to depend on neighboring blocks outputs, by showing how we can relabel the segmentation IDs to be consistent across blocks during the detection step. Let’s visualize an example block as though it were not completed, but its neighbors are done, and the read_roi is expanded by a certain amount of context.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>context = 10  # It could be as low as 1, but we use 10 for ease of visualization
seg_block_roi = daisy.Roi((128, 128), (128, 128))
seg_read_roi = seg_block_roi.grow(context, context)
seg_write_roi = seg_block_roi
seg_total_read_roi = total_roi.grow(context, context)

seg_block = daisy.Block(
    total_roi=seg_total_read_roi,
    read_roi=seg_read_roi,
    write_roi=seg_write_roi,
)

# simulate this block not being completed yet
blue_objs = zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;blue_objects&quot;][:]
blue_objs[128:356, 128:256] = 0

from skimage.color import label2rgb

figure, axes = plt.subplots(1, 2)
axes[0].imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;smoothed_for_seg&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
axes[1].imshow(blue_objs, origin=&quot;lower&quot;, cmap=colormap, interpolation=&quot;nearest&quot;)
display_roi(figure.axes[0], seg_block.read_roi, color=&quot;purple&quot;)
display_roi(figure.axes[0], seg_block.write_roi, color=&quot;white&quot;)
display_roi(figure.axes[1], seg_block.read_roi, color=&quot;purple&quot;)
display_roi(figure.axes[1], seg_block.write_roi, color=&quot;white&quot;)
</pre></div>
</div>
</div>
<p>Here the purple is the read_roi of our current block, and the white is the write_roi. As before, the process function will read the input image in the read_roi and segment it. From the previous result visualization, we can see that the function will detect the top half of the crest and assign it the label id that is visualized as brown.</p>
<p>Before we write out the brown label to the write_roi of the output dataset, however, we can adapt the process function to <strong>also read in the existing results in the output dataset</strong> in the read_roi. The existing green label will then overlap with the brown label, and our process function can relabel the top half of the crest green based on this information, before writing to the write_roi.</p>
<p>This approach only works if the overlapping blocks are run sequentially (and if objects don’t span across non-adjacent blocks - for large objects, you cannot relabel without a second pass). If the neighbors are run in parallel, it is possible that the green label will not yet be there when our current block reads existing labels, but also that our brown label will not yet be there when the block containing the green object reads existing labels. If <code class="docutils literal notranslate"><span class="pre">read_write_conflicts</span></code> argument is set to
True in a task, the daisy scheduler will ensure that pairs of blocks with overlapping read/write rois will never be run at the same time, thus avoiding this race condition.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># here is the new and improved segmentation function that reads in neighboring output context
def segment_blue_objects_with_context(input_group, output_group, block):
    import cv2
    from funlib.persistence.arrays import open_ds, Array
    import numpy as np
    import skimage
    import logging
    import time

    def get_overlapping_labels(array1, array2):
        &quot;&quot;&quot;A function to get all pairs of labels that intsersect between two arrays&quot;&quot;&quot;
        array1 = array1.flatten()
        array2 = array2.flatten()
        # get indices where both are not zero (ignore background)
        # this speeds up computation significantly
        non_zero_indices = np.logical_and(array1, array2)
        flattened_stacked = np.array(
            [array1[non_zero_indices], array2[non_zero_indices]]
        )
        intersections = np.unique(flattened_stacked, axis=1)
        return intersections  # a &lt;number of pairs&gt; x 2 nparray

    # load the data as usual
    input_ds = open_ds(
        f&quot;sample_data.zarr/{input_group}&quot;,
        &quot;r&quot;,
    )
    data = input_ds.to_ndarray(
        block.read_roi, fill_value=0
    )  # add the fill value because context

    # massage the data into open cv hsv format :/
    back_to_skimage = (data.transpose(1, 2, 0) * 255).astype(np.uint8)
    cv2_image = cv2.cvtColor(
        skimage.util.img_as_ubyte(back_to_skimage), cv2.COLOR_RGB2BGR
    )
    hsv_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2HSV)

    # Define the color range for detection
    lower_blue = np.array([100, 30, 0])
    upper_blue = np.array([150, 255, 255])

    # Threshold the image to get only blue colors
    mask = cv2.inRange(hsv_image, lower_blue, upper_blue)  # this returns 0 and 255
    mask = mask.astype(np.uint32)
    mask = mask // 255  # turn into 0/1 labels

    # give each connected component its own instance segmentation label
    labels = skimage.measure.label(mask)

    # get a unique ID for each element in the whole volume (avoid repeats between blocks)
    max_number_obj = (
        128 * 128
    )  # This number is an upper bound on the maximum number of objects in a block
    block_id_mask = mask * (block.block_id[1] * max_number_obj)
    labels = labels + block_id_mask

    # load the existing labels in the output
    output_ds = open_ds(f&quot;sample_data.zarr/{output_group}&quot;, &quot;a&quot;)
    existing_labels = output_ds.to_ndarray(block.read_roi, fill_value=0)

    # if there are existing labels, change the label to match
    # note: This only works if objects never span multiple rows/columns.
    # If you have long objects like neurons, you need to do true agglomeration
    intersections = get_overlapping_labels(labels, existing_labels)
    for index in range(intersections.shape[1]):
        label, existing_label = intersections[:, index]
        # Change the label to the one that was already in the neighbor
        labels[labels == label] = existing_label

    time.sleep(
        0.5
    )  # included to show that read_write_conflicts=False leads to race conditions

    # center crop and save the output dataset
    output_array = Array(labels, offset=block.read_roi.offset, voxel_size=(1, 1))
    output_ds[block.write_roi] = output_array.to_ndarray(block.write_roi)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>seg_block_roi = daisy.Roi((0, 0), (128, 128))
seg_block_read_roi = seg_block_roi.grow(context, context)

delete_ds(&quot;blue_objects_with_context&quot;)
prepare_ds(
    &quot;sample_data.zarr/blue_objects_with_context&quot;,
    shape=total_roi.shape,
    offset=total_roi.offset,
    voxel_size=daisy.Coordinate((1, 1)),
    dtype=np.uint32,
    write_size=seg_block_roi.shape,
)

seg_task = daisy.Task(
    &quot;segmentation_with_context&quot;,
    process_function=partial(
        segment_blue_objects_with_context,
        &quot;smoothed_for_seg&quot;,
        &quot;blue_objects_with_context&quot;,
    ),
    total_roi=total_read_roi,
    read_roi=seg_block_read_roi,
    write_roi=seg_block_roi,
    read_write_conflict=True,  # this ensures neighboring blocks aren&#39;t run at the same time
    num_workers=10,
    upstream_tasks=[smoothing_task],
)
daisy.run_blockwise([seg_task])
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>figure, axes = plt.subplots(1, 2)
axes[0].imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;smoothed_for_seg&quot;][:].transpose(1, 2, 0),
    origin=&quot;lower&quot;,
)
axes[1].imshow(
    zarr.open(&quot;sample_data.zarr&quot;, &quot;r&quot;)[&quot;blue_objects_with_context&quot;][:],
    cmap=colormap,
    interpolation=&quot;nearest&quot;,
    origin=&quot;lower&quot;,
)
</pre></div>
</div>
</div>
<p>All the labels are now consistent! If you re-run the previous cell with <code class="docutils literal notranslate"><span class="pre">read_write_conflict=False</span></code>, you should see an inconsistent result again due to the race conditions, even though the process function still reads the neighboring output.</p>
<p><strong>IMPORTANT PERFORMANCE NOTE:</strong> Be aware that <code class="docutils literal notranslate"><span class="pre">read_write_conflicts</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> by default and can lead to performance hits in cases where you don’t need it, so be sure to turn it off if you want every block to be run in parallel!</p>
</section>
</section>
<section id="Some-Minimal-Examples">
<h1>Some Minimal Examples<a class="headerlink" href="#Some-Minimal-Examples" title="Link to this heading"></a></h1>
<section id="Introduction-and-overview">
<h2>Introduction and overview<a class="headerlink" href="#Introduction-and-overview" title="Link to this heading"></a></h2>
<p>In this tutorial we will cover some basic Daisy <code class="docutils literal notranslate"><span class="pre">Task</span></code>s</p>
<p><strong>daisy.Task</strong>: All code you would like to distribute into blocks must be wrapped into a <code class="docutils literal notranslate"><span class="pre">Task</span></code> to be executed by daisy.</p>
</section>
<section id="Environment-setup">
<h2>Environment setup<a class="headerlink" href="#Environment-setup" title="Link to this heading"></a></h2>
<p>If you have not already done so, I highly recommend you create an environment first. You can do this with <code class="docutils literal notranslate"><span class="pre">uv</span></code> via:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>init<span class="w"> </span>--python<span class="w"> </span><span class="m">3</span>.12
</pre></div>
</div>
<p>Activating the environment is as simple as <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">.venv/bin/activate</span></code></p>
<p>Then, you can:</p>
<ul class="simple">
<li><p>add daisy directly to your dependencies with: <code class="docutils literal notranslate"><span class="pre">bash</span>&#160;&#160;&#160;&#160; <span class="pre">uv</span> <span class="pre">add</span> <span class="pre">daisy</span></code></p></li>
<li><p>install daisy using pip by name or via GitHub: <code class="docutils literal notranslate"><span class="pre">bash</span>&#160;&#160;&#160;&#160; <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">daisy</span></code></p></li>
</ul>
<section id="Multiprocessing">
<h3>Multiprocessing<a class="headerlink" href="#Multiprocessing" title="Link to this heading"></a></h3>
<p>Here we set the start method to fork. We do this to simplify running this notebook in a jupyter notebook. The “spawn” start method is supported but limits the possible functions you can execute blockwise (i.e. no lambda functions)</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import multiprocessing

multiprocessing.set_start_method(&quot;fork&quot;, force=True)
</pre></div>
</div>
</div>
</section>
</section>
<section id="Daisy-Tasks">
<h2>Daisy Tasks<a class="headerlink" href="#Daisy-Tasks" title="Link to this heading"></a></h2>
<section id="A-Simple-Task">
<h3>A Simple Task<a class="headerlink" href="#A-Simple-Task" title="Link to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import daisy
from daisy import Coordinate, Roi
import time

# Create a super simple task
dummy_task = daisy.Task(
    &quot;Dummy&quot;,  # We give the task a name
    total_roi=Roi((0,), (100,)),  # a 1-D bounding box [0,100)
    read_roi=Roi((0,), (1,)),  # We read in blocks of size [0,1)
    write_roi=Roi((0,), (1,)),  # We write in blocks of size [0, 1)
    process_function=lambda block: time.sleep(
        0.05
    ),  # Our process function takes the block and simply waits
    num_workers=5,
)

# execute the task without any multiprocessing
daisy.run_blockwise([dummy_task], multiprocessing=False)
daisy.run_blockwise([dummy_task], multiprocessing=True)
</pre></div>
</div>
</div>
<p>Since there are 100 blocks to process, and we always sleep for 0.05 seconds, it takes 5 seconds to run all the blocks without multiprocessing, but only 1 second with <code class="docutils literal notranslate"><span class="pre">multiprocessing=True</span></code>. When you run a task with <code class="docutils literal notranslate"><span class="pre">daisy</span></code>, you get a progress bar along with a summary of the execution after you have finished processing. Here no errors were thrown so we completed all 100 blocks.</p>
<p><code class="docutils literal notranslate"><span class="pre">daisy</span></code> also programatically returns a dictionary containing a summary of the execution for all given tasks so that you can decide how to procede based on whether any blocks failed.</p>
<p>Next lets look see what we can do to modify this task</p>
</section>
<section id="Failing-blocks">
<h3>Failing blocks<a class="headerlink" href="#Failing-blocks" title="Link to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><br/><span></span>import random


def random_fail(block: daisy.Block):
    if random.random() &lt; 0.2:
        block.status = daisy.BlockStatus.FAILED
    return block


# Create a super simple task
failing_task = daisy.Task(
    &quot;Failures&quot;,  # We give the task a name
    total_roi=Roi((0,), (100,)),  # a 1-D bounding box [0,100)
    read_roi=Roi((0,), (1,)),  # We read in blocks of size [0,1)
    write_roi=Roi((0,), (1,)),  # We write in blocks of size [0, 1)
    process_function=random_fail,  # Our process function takes the block and simply waits
    max_retries=0,  # We do not retry failed blocks to get expected failure counts
)

# execute the task without any multiprocessing
daisy.run_blockwise([failing_task], multiprocessing=False)
</pre></div>
</div>
</div>
<p>In this case, we have a 20% chance of failing each block. As you see from the execution summary, about 20% of the blocks failed. If your blocks are failing, you may want to rerun the task and only process failed blocks. This is where we use the <code class="docutils literal notranslate"><span class="pre">check_function</span></code> argument in the <code class="docutils literal notranslate"><span class="pre">daisy.Task</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import tempfile
from pathlib import Path

with tempfile.TemporaryDirectory() as tmpdir:

    def random_fail(block: daisy.Block):
        if random.random() &lt; 0.2:
            block.status = daisy.BlockStatus.FAILED
        else:
            Path(tmpdir, f&quot;{block.block_id[1]}&quot;).touch()
        return block

    def check_block(block: daisy.Block) -&gt; bool:
        return Path(tmpdir, f&quot;{block.block_id[1]}&quot;).exists()

    while True:
        # Create a super simple task
        check_block_task = daisy.Task(
            &quot;Checking-Blocks&quot;,  # We give the task a name
            total_roi=Roi((0,), (100,)),  # a 1-D bounding box [0,100)
            read_roi=Roi((0,), (1,)),  # We read in blocks of size [0,1)
            write_roi=Roi((0,), (1,)),  # We write in blocks of size [0, 1)
            process_function=random_fail,  # Our process function takes the block and simply waits
            check_function=check_block,  # Check if a block has been completed or not
            max_retries=0,
        )

        # execute the task without any multiprocessing
        task_state = daisy.run_blockwise([check_block_task], multiprocessing=False)
        if task_state[&quot;Checking-Blocks&quot;].failed_count == 0:
            break
</pre></div>
</div>
</div>
<p>It took a few tries to complete the task since approximately 20% of the remaining blocks fail on each attempt. We could also have set <code class="docutils literal notranslate"><span class="pre">max_retries</span></code> to a higher number to allow for retrying failed blocks during the same task execution. This is to account for random failures due to network issues or other random uncontrollable factors, but will not help if there is a logical bug in your code. In this case since the failure is random, retrying does help.</p>
</section>
<section id="Different-read/write-sizes-and-boundary-handling">
<h3>Different read/write sizes and boundary handling<a class="headerlink" href="#Different-read/write-sizes-and-boundary-handling" title="Link to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><br/><span></span># fit: &quot;valid&quot;, &quot;overhang&quot;, &quot;shrink&quot;

# Create a super simple task
overhang_task = daisy.Task(
    &quot;overhang&quot;,  # We give the task a name
    total_roi=Roi((0,), (100,)),  # a 1-D bounding box [0,100)
    read_roi=Roi((0,), (9,)),  # We read in blocks of size [0,1)
    write_roi=Roi((2,), (5,)),  # We write in blocks of size [0, 1)
    process_function=lambda b: ...,  # Empty process function
    fit=&quot;overhang&quot;,
)
valid_task = daisy.Task(
    &quot;valid&quot;,  # We give the task a name
    total_roi=Roi((0,), (100,)),  # a 1-D bounding box [0,100)
    read_roi=Roi((0,), (9,)),  # We read in blocks of size [0,1)
    write_roi=Roi((2,), (5,)),  # We write in blocks of size [0, 1)
    process_function=lambda b: ...,  # Empty process function
    fit=&quot;valid&quot;,
)
</pre></div>
</div>
</div>
<p>because we now have:</p>
<p>total roi: [0, 100)</p>
<p>read roi: [0, 9)</p>
<p>write roi: [2, 7)</p>
<p>There is a “context” of 2 pixels between the read_roi and write roi. Removing this context from the total roi gives us a “total write roi” of [2, 98). Note that the total number of pixels to tile is 96, which is not evenly divisible by the chunk size of 5. So our final block will have write roi of [97, 102) and a read roi of [95, 104). Depending on your task, reading, and especially writing outside of the given total roi bounds can cause problems. in these cases we can handle the final block in
a few ways. “valid”: ignore the final block. “overhang”: process the final block. “shrink”: shrink the read/write rois until the read roi is fully contained in the total roi. In this case the “shrink” strategy would shrink the read roi to [95, 100) and the write roi to [97, 98).</p>
<p>Now when we executing the overhang task it will have one more block processed than the valid task</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>daisy.run_blockwise([overhang_task], multiprocessing=False)
daisy.run_blockwise([valid_task], multiprocessing=False)
</pre></div>
</div>
</div>
</section>
<section id="Task-Chaining">
<h3>Task Chaining<a class="headerlink" href="#Task-Chaining" title="Link to this heading"></a></h3>
<p>It is also possible to chain multiple tasks together in such a way that they either run in parallel or in sequence. If running in parallel, we simply start the workers for both tasks at once. If running in sequence, blocks will only be realeased once their dependencies in previous tasks have been completed.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>total_roi = Roi((0, 0), (1250, 1250))
read_roi_shape = (400, 400)
write_roi_shape = (250, 250)
context = Coordinate(75, 75)

task_a_to_b = daisy.Task(
    &quot;A_to_B&quot;,
    total_roi=total_roi.grow(
        context,
        context,
    ),
    read_roi=Roi((0, 0), read_roi_shape),
    write_roi=Roi(context, write_roi_shape),
    process_function=lambda b: ...,
    read_write_conflict=False,
)
task_b_to_c = daisy.Task(
    &quot;B_to_C&quot;,
    total_roi=total_roi.grow(
        context,
        context,
    ),
    read_roi=Roi((0, 0), read_roi_shape),
    write_roi=Roi(context, write_roi_shape),
    process_function=lambda b: ...,
    read_write_conflict=False,
    upstream_tasks=[task_a_to_b],
)

daisy.run_blockwise([task_a_to_b, task_b_to_c], multiprocessing=False)
</pre></div>
</div>
</div>
<p>Here is an mp4 visualization of the above task</p>
<img alt="task chaining" src="_static/task_chaining.gif" />
</section>
</section>
</section>
<section id="Map-Reduce-and-Some-simple-benchmarks">
<h1>Map Reduce and Some simple benchmarks<a class="headerlink" href="#Map-Reduce-and-Some-simple-benchmarks" title="Link to this heading"></a></h1>
<section id="Simple-data">
<h2>Simple data<a class="headerlink" href="#Simple-data" title="Link to this heading"></a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import timeit

shape = 4096000

a = np.arange(shape, dtype=np.int64)
b = np.empty_like(a, dtype=np.int64)
print(f&quot;Array a: {a[:6]} ...&quot;)
</pre></div>
</div>
</div>
</section>
<section id="Map">
<h2>Map<a class="headerlink" href="#Map" title="Link to this heading"></a></h2>
<p>Here we will square each element in the array <code class="docutils literal notranslate"><span class="pre">a</span></code> and store the result in a new array <code class="docutils literal notranslate"><span class="pre">b</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>def process_fn():
    # iterating and squaring each element in a and store to b
    with np.nditer([a, b], op_flags=[[&quot;readonly&quot;], [&quot;readwrite&quot;]]) as it:
        with it:
            for x, y in it:
                y[...] = x**2


print(f&quot;Squaring array a took {timeit.timeit(process_fn, number=1)} seconds&quot;)
print(f&quot;Array b: {b[:6]} ...&quot;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import daisy
from funlib.persistence import Array
import zarr

shape = 4096000
block_shape = 1024 * 16

# input array is wrapped in Array for easy of Roi indexing
a = Array(
    np.arange(shape, dtype=np.int64),
    offset=(0,),
    voxel_size=(1,),
)

# to parallelize across processes, we need persistent read/write arrays
# we&#39;ll use zarr here to do do that
b = zarr.open_array(
    zarr.TempStore(), &quot;w&quot;, (shape,), chunks=(block_shape,), dtype=np.int64
)
# output array is wrapped in Array for easy of Roi indexing
b = Array(
    b,
    offset=(0,),
    voxel_size=(1,),
)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># same process function as previously, but with additional code
# to read and write data to persistent arrays
def process_fn_daisy(block):
    a_sub = a[block.read_roi]
    b_sub = np.empty_like(a_sub)
    with np.nditer(
        [a_sub, b_sub],
        op_flags=[[&quot;readonly&quot;], [&quot;readwrite&quot;]],
    ) as it:
        with it:
            for x, y in it:
                y[...] = x**2

    b[block.write_roi] = b_sub
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>total_roi = daisy.Roi((0,), shape)  # total ROI to map process over
block_roi = daisy.Roi((0,), (block_shape,))  # block ROI for parallel processing

# creating a Daisy task, note that we do not specify how each
# worker should read/write to input/output arrays
task = daisy.Task(
    total_roi=total_roi,
    read_roi=block_roi,
    write_roi=block_roi,
    process_function=process_fn_daisy,
    num_workers=8,
    task_id=&quot;square&quot;,
)

daisy.run_blockwise([task])
print(f&quot;Array b: {b[:6]} ...&quot;)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span># run this to benchmark daisy!
run_daisy = lambda: daisy.run_blockwise([task])
print(f&quot;Squaring array a with daisy took {timeit.timeit(run_daisy, number=1)} seconds&quot;)
</pre></div>
</div>
</div>
</section>
<section id="Reduce">
<h2>Reduce<a class="headerlink" href="#Reduce" title="Link to this heading"></a></h2>
<p>Here we will take the sum over every 16 elements in the array <code class="docutils literal notranslate"><span class="pre">b</span></code> and store the result in a new array <code class="docutils literal notranslate"><span class="pre">c</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre><span></span>import multiprocessing

reduce_shape = shape / 16

# while using zarr with daisy.Array can be easier to understand and less error prone, it is not a requirement.
# Here we make a shared memory array for collecting results from different workers
c = multiprocessing.Array(&quot;Q&quot;, range(int(shape / reduce_shape)))


def process_fn_sum_reduce(block):
    b_sub = b[block.write_roi]
    s = np.sum(b_sub)
    # compute c idx based on block offset and shape
    idx = (block.write_roi.offset / block.write_roi.shape)[0]
    c[idx] = s


total_roi = daisy.Roi((0,), shape)  # total ROI to map process over
block_roi = daisy.Roi((0,), reduce_shape)  # block ROI for parallel processing

task1 = daisy.Task(
    total_roi=total_roi,
    read_roi=block_roi,
    write_roi=block_roi,
    process_function=process_fn_sum_reduce,
    num_workers=8,
    task_id=&quot;sum_reduce&quot;,
)

daisy.run_blockwise([task1])
print(c[:])
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Daisy: A Blockwise Task Scheduler" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="api.html" class="btn btn-neutral float-right" title="API Reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Jan Funke, Tri Nguyen, William Patton, Caroline Malin-Mayor, Arlo Sheridan, Philipp Hanslovsky, Chris Barnes.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>